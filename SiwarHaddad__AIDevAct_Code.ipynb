{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AIDev Dataset Analysis Pipeline\n",
        "\n",
        " **Research Goal**: Analyze the quality and maintainability characteristics of AI-generated\n",
        "\n",
        " **Research Questions**:\n",
        " - RQ1: How do AI-generated code contributions differ from human contributions in structural quality?\n",
        " - RQ2: What is the relationship between AI-generated code and post-merge maintenance effort?\n",
        " - RQ3: Which characteristics of Agentic-PRs predict successful integration?\n",
        "\n",
        " **Dataset**: AIDev-pop (7,122 Agentic-PRs from 856 repos >500 stars) + Human-PRs baseline\n",
        "\n"
      ],
      "metadata": {
        "id": "UpnDZf1Ab9IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q pandas numpy scipy scikit-learn matplotlib seaborn\n",
        "\n",
        "# %% Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import mannwhitneyu, chi2_contingency\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n",
        "import re\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization defaults\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "id": "RtYEPo3QcTSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Define AIDevAnalyzer class\n",
        "class AIDevAnalyzer:\n",
        "    \"\"\"\n",
        "    Main analyzer class for AIDev dataset research pipeline.\n",
        "    Implements GQM approach for RQ1, RQ2, and RQ3.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_path='./outputs'):\n",
        "        \"\"\"Initialize analyzer with output directory.\"\"\"\n",
        "        self.output_path = output_path\n",
        "\n",
        "        # Core tables\n",
        "        self.pull_requests = None\n",
        "        self.repositories = None\n",
        "        self.users = None\n",
        "        self.pr_commits = None\n",
        "        self.pr_commit_details = None\n",
        "        self.pr_reviews = None\n",
        "        self.pr_comments = None\n",
        "        self.pr_task_type = None\n",
        "        self.related_issues = None\n",
        "        self.issues = None\n",
        "        self.pr_timeline = None\n",
        "\n",
        "        # Human baseline\n",
        "        self.human_pr = None\n",
        "        self.human_pr_task_type = None\n",
        "\n",
        "        # Analysis results\n",
        "        self.metrics_df = None\n",
        "        self.combined_df = None\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"AIDevAnalyzer Initialized\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Output path: {self.output_path}\\n\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load AIDev-pop dataset and human baseline from Hugging Face.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"STEP 1: LOADING DATASET\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        path = \"hf://datasets/hao-li/AIDev/\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n[1/12] Loading pull_request (pop)...\")\n",
        "            self.pull_requests = pd.read_parquet(path + 'pull_request.parquet')\n",
        "\n",
        "            print(\"[2/12] Loading repository (pop)...\")\n",
        "            self.repositories = pd.read_parquet(path + 'repository.parquet')\n",
        "\n",
        "            print(\"[3/12] Loading user (pop)...\")\n",
        "            self.users = pd.read_parquet(path + 'user.parquet')\n",
        "\n",
        "            print(\"[4/12] Loading pr_commits (pop)...\")\n",
        "            self.pr_commits = pd.read_parquet(path + 'pr_commits.parquet')\n",
        "\n",
        "            print(\"[5/12] Loading pr_commit_details (pop)...\")\n",
        "            self.pr_commit_details = pd.read_parquet(path + 'pr_commit_details.parquet')\n",
        "\n",
        "            print(\"[6/12] Loading pr_reviews (pop)...\")\n",
        "            self.pr_reviews = pd.read_parquet(path + 'pr_reviews.parquet')\n",
        "\n",
        "            print(\"[7/12] Loading pr_comments (pop)...\")\n",
        "            self.pr_comments = pd.read_parquet(path + 'pr_comments.parquet')\n",
        "\n",
        "            print(\"[8/12] Loading pr_task_type (pop)...\")\n",
        "            self.pr_task_type = pd.read_parquet(path + 'pr_task_type.parquet')\n",
        "\n",
        "            print(\"[9/12] Loading related_issue (pop)...\")\n",
        "            self.related_issues = pd.read_parquet(path + 'related_issue.parquet')\n",
        "\n",
        "            print(\"[10/12] Loading issue (pop)...\")\n",
        "            self.issues = pd.read_parquet(path + 'issue.parquet')\n",
        "\n",
        "            print(\"[11/12] Loading pr_timeline (pop)...\")\n",
        "            self.pr_timeline = pd.read_parquet(path + 'pr_timeline.parquet')\n",
        "\n",
        "            print(\"[12/12] Loading human_pull_request (base)...\")\n",
        "            self.human_pr = pd.read_parquet(path + 'human_pull_request.parquet')\n",
        "            self.human_pr_task_type = pd.read_parquet(path + 'human_pr_task_type.parquet')\n",
        "\n",
        "            print(\"\\n✓ All tables loaded successfully!\")\n",
        "            self._print_dataset_overview()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n✗ Error loading data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _print_dataset_overview(self):\n",
        "        \"\"\"Print summary statistics of loaded dataset.\"\"\"\n",
        "        print(\"\\n\" + \"-\" * 70)\n",
        "        print(\"DATASET OVERVIEW\")\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"Agentic PRs (pop): {len(self.pull_requests):,}\")\n",
        "        print(f\"Human PRs (base): {len(self.human_pr):,}\")\n",
        "        print(f\"Repositories: {len(self.repositories):,}\")\n",
        "        print(f\"Users: {len(self.users):,}\")\n",
        "        print(f\"Commits: {len(self.pr_commits):,}\")\n",
        "        print(f\"Commit Details: {len(self.pr_commit_details):,}\")\n",
        "        print(f\"Reviews: {len(self.pr_reviews):,}\")\n",
        "        print(f\"Comments: {len(self.pr_comments):,}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    def apply_inclusion_exclusion_criteria(self, min_stars=500):\n",
        "        \"\"\"\n",
        "        Apply inclusion/exclusion criteria to create filtered dataset.\n",
        "\n",
        "        Inclusion:\n",
        "        - Repos with >= min_stars\n",
        "        - PRs from Jan 2025 - June 2025\n",
        "        - Complete metadata (commits available)\n",
        "        - Clear author attribution\n",
        "\n",
        "        Exclusion:\n",
        "        - Incomplete metadata\n",
        "        - Non-agent bots\n",
        "        - Ambiguous attribution\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"STEP 2: APPLYING INCLUSION/EXCLUSION CRITERIA\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Filter Agentic PRs\n",
        "        initial_agent = len(self.pull_requests)\n",
        "        print(f\"\\nInitial Agentic PRs: {initial_agent:,}\")\n",
        "\n",
        "        # Confirm min stars\n",
        "        high_star_repos = self.repositories[\n",
        "            self.repositories['stars'] >= min_stars\n",
        "        ]['id'].values\n",
        "        self.pull_requests = self.pull_requests[\n",
        "            self.pull_requests['repo_id'].isin(high_star_repos)\n",
        "        ]\n",
        "        print(f\"  → After star filter: {len(self.pull_requests):,} PRs\")\n",
        "\n",
        "        # Date filter\n",
        "        self.pull_requests['created_at'] = pd.to_datetime(self.pull_requests['created_at'])\n",
        "        date_mask = (\n",
        "            (self.pull_requests['created_at'] >= '2025-01-01') &\n",
        "            (self.pull_requests['created_at'] <= '2025-06-22')\n",
        "        )\n",
        "        self.pull_requests = self.pull_requests[date_mask]\n",
        "        print(f\"  → After date filter: {len(self.pull_requests):,} PRs\")\n",
        "\n",
        "        # Complete metadata\n",
        "        complete_prs = self.pr_commits['pr_id'].unique()\n",
        "        self.pull_requests = self.pull_requests[\n",
        "            self.pull_requests['id'].isin(complete_prs)\n",
        "        ]\n",
        "        print(f\"  → After completeness filter: {len(self.pull_requests):,} PRs\")\n",
        "\n",
        "        # Filter Human PRs to same repos\n",
        "        initial_human = len(self.human_pr)\n",
        "        print(f\"\\nInitial Human PRs: {initial_human:,}\")\n",
        "\n",
        "\n",
        "        self.human_pr['created_at'] = pd.to_datetime(self.human_pr['created_at'])\n",
        "        human_date_mask = (\n",
        "            (self.human_pr['created_at'] >= '2025-01-01') &\n",
        "            (self.human_pr['created_at'] <= '2025-06-22')\n",
        "        )\n",
        "        self.human_pr = self.human_pr[human_date_mask]\n",
        "        print(f\"  → After filters: {len(self.human_pr):,} PRs\")\n",
        "\n",
        "        # Combine datasets\n",
        "        self.pull_requests['author_type'] = 'Agent'\n",
        "        self.human_pr['author_type'] = 'Human'\n",
        "        self.human_pr['agent'] = 'Human'\n",
        "\n",
        "        self.combined_df = pd.concat(\n",
        "            [self.pull_requests, self.human_pr],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✓ Final combined dataset: {len(self.combined_df):,} PRs\")\n",
        "        print(f\"  - Agent: {len(self.pull_requests):,} ({len(self.pull_requests)/len(self.combined_df)*100:.1f}%)\")\n",
        "        print(f\"  - Human: {len(self.human_pr):,} ({len(self.human_pr)/len(self.combined_df)*100:.1f}%)\")\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        \"\"\"Calculate all metrics for RQ1, RQ2, RQ3.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"STEP 3: CALCULATING METRICS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Merge task types\n",
        "        print(\"\\nMerging task types...\")\n",
        "        agent_tasks = self.pr_task_type.copy()\n",
        "        agent_tasks['author_type'] = 'Agent'\n",
        "        human_tasks = self.human_pr_task_type.copy()\n",
        "        human_tasks['author_type'] = 'Human'\n",
        "        all_tasks = pd.concat([agent_tasks, human_tasks], ignore_index=True)\n",
        "\n",
        "        # Merge with main dataset\n",
        "        self.metrics_df = self.combined_df.merge(\n",
        "            all_tasks[['id', 'agent', 'type', 'author_type']],\n",
        "            on=['id', 'agent', 'author_type'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        self.metrics_df['repo_id'] = pd.to_numeric(self.metrics_df['repo_id'], errors='coerce').astype('Int64')\n",
        "        self.repositories['id'] = pd.to_numeric(self.repositories['id'], errors='coerce').astype('Int64')\n",
        "\n",
        "        # Keep all PRs, even if repo metadata is missing\n",
        "        self.metrics_df = self.metrics_df.merge(\n",
        "            self.repositories[['id', 'language', 'stars', 'forks']],\n",
        "            left_on='repo_id',\n",
        "            right_on='id',\n",
        "            how='left',\n",
        "            suffixes=('', '_repo')\n",
        "        )\n",
        "\n",
        "        # Preprocessing timestamps\n",
        "        print(\"Processing timestamps...\")\n",
        "        self.metrics_df['created_at'] = pd.to_datetime(self.metrics_df['created_at'])\n",
        "        self.metrics_df['closed_at'] = pd.to_datetime(self.metrics_df['closed_at'], errors='coerce')\n",
        "        self.metrics_df['merged_at'] = pd.to_datetime(self.metrics_df['merged_at'], errors='coerce')\n",
        "\n",
        "        # Aggregate commit details per PR\n",
        "        print(\"Aggregating commit details...\")\n",
        "        commit_agg = self.pr_commit_details.groupby('pr_id').agg({\n",
        "            'additions': 'sum',\n",
        "            'deletions': 'sum',\n",
        "            'filename': 'nunique',\n",
        "            'changes': 'sum'\n",
        "        }).reset_index().rename(columns={'filename': 'files_touched'})\n",
        "\n",
        "        self.metrics_df = self.metrics_df.merge(\n",
        "            commit_agg,\n",
        "            left_on='id',\n",
        "            right_on='pr_id',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        human_mask = self.metrics_df['author_type'] == 'Human'\n",
        "        self.metrics_df.loc[human_mask, ['additions', 'deletions', 'files_touched']] = \\\n",
        "            self.metrics_df.loc[human_mask, ['additions', 'deletions', 'files_touched']].fillna(0)\n",
        "\n",
        "\n",
        "        # Fill NaN values\n",
        "        self.metrics_df.fillna({\n",
        "            'additions': 0,\n",
        "            'deletions': 0,\n",
        "            'files_touched': 0,\n",
        "            'changes': 0\n",
        "        }, inplace=True)\n",
        "\n",
        "        # RQ1 Metrics: Code Quality\n",
        "        print(\"Calculating RQ1 metrics (code quality)...\")\n",
        "        self.metrics_df['loc_changed'] = self.metrics_df['additions'] + self.metrics_df['deletions']\n",
        "        self.metrics_df['add_del_ratio'] = np.where(\n",
        "            self.metrics_df['deletions'] > 0,\n",
        "            self.metrics_df['additions'] / self.metrics_df['deletions'],\n",
        "            self.metrics_df['additions']\n",
        "        )\n",
        "\n",
        "        # Complexity score (proxy)\n",
        "        self.metrics_df['complexity_score'] = (\n",
        "            self.metrics_df['additions'] * 0.1 +\n",
        "            self.metrics_df['files_touched'] * 2\n",
        "        )\n",
        "\n",
        "        # Change dispersion\n",
        "        self.metrics_df['change_dispersion'] = np.where(\n",
        "            self.metrics_df['loc_changed'] > 0,\n",
        "            self.metrics_df['files_touched'] / (self.metrics_df['loc_changed'] + 1),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Test file detection (basic heuristic)\n",
        "        test_keywords = ['test', 'spec', '__tests__', '.test.', '.spec.']\n",
        "        def has_test_files(pr_id):\n",
        "            if pr_id not in self.pr_commit_details['pr_id'].values:\n",
        "                return 0\n",
        "            files = self.pr_commit_details[\n",
        "                self.pr_commit_details['pr_id'] == pr_id\n",
        "            ]['filename'].values\n",
        "            return int(any(any(kw in str(f).lower() for kw in test_keywords) for f in files))\n",
        "\n",
        "        print(\"  - Detecting test files...\")\n",
        "        self.metrics_df['has_tests'] = self.metrics_df['id'].apply(has_test_files)\n",
        "\n",
        "        # RQ2 Metrics: Maintenance Effort\n",
        "        print(\"Calculating RQ2 metrics (maintenance effort)...\")\n",
        "        self.metrics_df['is_accepted'] = (~self.metrics_df['merged_at'].isna()).astype(int)\n",
        "        self.metrics_df['is_closed'] = (~self.metrics_df['closed_at'].isna()).astype(int)\n",
        "\n",
        "        # Review time (hours)\n",
        "        self.metrics_df['review_time_hours'] = (\n",
        "            self.metrics_df['closed_at'] - self.metrics_df['created_at']\n",
        "        ).dt.total_seconds() / 3600\n",
        "        self.metrics_df['review_time_hours'] = self.metrics_df['review_time_hours'].fillna(0)\n",
        "\n",
        "        # Review and comment counts\n",
        "        review_counts = self.pr_reviews.groupby('pr_id').size().reset_index(name='num_reviews')\n",
        "        comment_counts = self.pr_comments.groupby('pr_id').size().reset_index(name='num_comments_total')\n",
        "\n",
        "        self.metrics_df = self.metrics_df.merge(review_counts, left_on='id', right_on='pr_id', how='left', suffixes=('', '_rev_dup'))\n",
        "        self.metrics_df = self.metrics_df.merge(comment_counts, left_on='id', right_on='pr_id', how='left', suffixes=('', '_com_dup'))\n",
        "\n",
        "        # Combine counts safely\n",
        "        self.metrics_df['num_comments'] = (\n",
        "            self.metrics_df['num_reviews'].fillna(0) +\n",
        "            self.metrics_df['num_comments_total'].fillna(0)\n",
        "        )\n",
        "\n",
        "        # Linked issues (proxy for bug fixes)\n",
        "        issue_links = self.related_issues.groupby('pr_id').size().reset_index(name='linked_issues')\n",
        "        self.metrics_df = self.metrics_df.merge(issue_links, left_on='id', right_on='pr_id', how='left')\n",
        "        self.metrics_df['linked_issues'] = self.metrics_df['linked_issues'].fillna(0)\n",
        "\n",
        "        # Bug-fix indicator\n",
        "        self.metrics_df['is_bug_fix'] = (\n",
        "            self.metrics_df['type'].isin(['fix', 'perf'])\n",
        "        ).astype(int)\n",
        "\n",
        "        # RQ3 Metrics: PR Characteristics\n",
        "        print(\"Calculating RQ3 metrics (success predictors)...\")\n",
        "        self.metrics_df['title_length'] = self.metrics_df['title'].str.len().fillna(0)\n",
        "        self.metrics_df['body_length'] = self.metrics_df['body'].str.len().fillna(0)\n",
        "        self.metrics_df['has_description'] = (self.metrics_df['body_length'] > 0).astype(int)\n",
        "\n",
        "        # Clean up\n",
        "        self.metrics_df.drop(columns=['pr_id'], errors='ignore', inplace=True)\n",
        "\n",
        "        print(f\"\\n✓ Metrics calculated for {len(self.metrics_df):,} PRs\")\n",
        "        print(f\"  Total features: {len(self.metrics_df.columns)}\")\n",
        "\n",
        "    def analyze_rq1(self):\n",
        "        \"\"\"RQ1: Structural code quality differences.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ANALYZING RQ1: STRUCTURAL CODE QUALITY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        agent = self.metrics_df[self.metrics_df['author_type'] == 'Agent']\n",
        "        human = self.metrics_df[self.metrics_df['author_type'] == 'Human']\n",
        "\n",
        "        # Statistical tests\n",
        "        results = []\n",
        "\n",
        "        print(\"\\n[RQ1.1] Code Complexity Comparison:\")\n",
        "        for metric in ['loc_changed', 'files_touched', 'additions', 'deletions',\n",
        "                       'complexity_score', 'change_dispersion']:\n",
        "            stat, p = mannwhitneyu(agent[metric].dropna(), human[metric].dropna())\n",
        "\n",
        "            # Cliff's Delta\n",
        "            n1, n2 = len(agent[metric].dropna()), len(human[metric].dropna())\n",
        "            delta = (stat - n1 * n2 / 2) / (n1 * n2)\n",
        "\n",
        "            results.append({\n",
        "                'Metric': metric,\n",
        "                'Agent_Median': agent[metric].median(),\n",
        "                'Human_Median': human[metric].median(),\n",
        "                'U_Statistic': stat,\n",
        "                'p_value': p,\n",
        "                'Cliff_Delta': delta,\n",
        "                'Significant': p < 0.05\n",
        "            })\n",
        "\n",
        "            print(f\"  {metric}: Agent={agent[metric].median():.1f}, \"\n",
        "                  f\"Human={human[metric].median():.1f}, p={p:.4f}\")\n",
        "\n",
        "        print(\"\\n[RQ1.2] Testing Practices:\")\n",
        "        # Chi-square for has_tests\n",
        "        contingency = pd.crosstab(\n",
        "            self.metrics_df['author_type'],\n",
        "            self.metrics_df['has_tests']\n",
        "        )\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
        "\n",
        "        agent_test_pct = (agent['has_tests'].sum() / len(agent)) * 100\n",
        "        human_test_pct = (human['has_tests'].sum() / len(human)) * 100\n",
        "\n",
        "        print(f\"  Agent with tests: {agent_test_pct:.1f}%\")\n",
        "        print(f\"  Human with tests: {human_test_pct:.1f}%\")\n",
        "        print(f\"  Chi-square: χ²={chi2:.2f}, p={p:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': 'has_tests',\n",
        "            'Agent_Median': agent_test_pct,\n",
        "            'Human_Median': human_test_pct,\n",
        "            'U_Statistic': chi2,\n",
        "            'p_value': p,\n",
        "            'Cliff_Delta': None,\n",
        "            'Significant': p < 0.05\n",
        "        })\n",
        "\n",
        "        print(\"\\n[RQ1.3] Task Type Distribution:\")\n",
        "        task_dist = pd.crosstab(\n",
        "            self.metrics_df['author_type'],\n",
        "            self.metrics_df['type'],\n",
        "            normalize='index'\n",
        "        ) * 100\n",
        "        print(task_dist.round(1))\n",
        "\n",
        "        # Save results\n",
        "        rq1_df = pd.DataFrame(results)\n",
        "        rq1_df.to_csv(f'{self.output_path}rq1_results.csv', index=False)\n",
        "        print(f\"\\n✓ RQ1 results saved to {self.output_path}rq1_results.csv\")\n",
        "\n",
        "        # Visualizations\n",
        "        self._visualize_rq1()\n",
        "\n",
        "    def _visualize_rq1(self):\n",
        "        \"\"\"Generate RQ1 visualizations.\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # LOC Changed\n",
        "        sns.boxplot(data=self.metrics_df, x='author_type', y='loc_changed', ax=axes[0,0])\n",
        "        axes[0,0].set_title('Lines of Code Changed')\n",
        "        axes[0,0].set_ylim(0, 500)\n",
        "\n",
        "        # Files Touched\n",
        "        sns.boxplot(data=self.metrics_df, x='author_type', y='files_touched', ax=axes[0,1])\n",
        "        axes[0,1].set_title('Files Modified per PR')\n",
        "        axes[0,1].set_ylim(0, 20)\n",
        "\n",
        "        # Complexity Score\n",
        "        sns.boxplot(data=self.metrics_df, x='author_type', y='complexity_score', ax=axes[0,2])\n",
        "        axes[0,2].set_title('Complexity Score')\n",
        "        axes[0,2].set_ylim(0, 100)\n",
        "\n",
        "        # Testing Rate\n",
        "        test_rates = self.metrics_df.groupby('author_type')['has_tests'].mean() * 100\n",
        "        test_rates.plot(kind='bar', ax=axes[1,0])\n",
        "        axes[1,0].set_title('PRs with Tests (%)')\n",
        "        axes[1,0].set_ylabel('Percentage')\n",
        "        axes[1,0].set_xlabel('Author Type')\n",
        "\n",
        "        # Task Type Distribution\n",
        "        task_counts = pd.crosstab(\n",
        "            self.metrics_df['author_type'],\n",
        "            self.metrics_df['type']\n",
        "        )\n",
        "        task_counts.T.plot(kind='bar', ax=axes[1,1])\n",
        "        axes[1,1].set_title('Task Type Distribution')\n",
        "        axes[1,1].legend(title='Author Type')\n",
        "        axes[1,1].set_xlabel('Task Type')\n",
        "\n",
        "        # Change Dispersion\n",
        "        sns.boxplot(data=self.metrics_df, x='author_type', y='change_dispersion', ax=axes[1,2])\n",
        "        axes[1,2].set_title('Change Dispersion')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.output_path}rq1_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Visualizations saved to {self.output_path}rq1_visualizations.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_rq2(self):\n",
        "        \"\"\"RQ2: Post-merge maintenance effort.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ANALYZING RQ2: MAINTENANCE EFFORT\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        agent = self.metrics_df[self.metrics_df['author_type'] == 'Agent']\n",
        "        human = self.metrics_df[self.metrics_df['author_type'] == 'Human']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        print(\"\\n[RQ2.1] Acceptance Rates:\")\n",
        "        # Chi-square for acceptance\n",
        "        contingency = pd.crosstab(\n",
        "            self.metrics_df['author_type'],\n",
        "            self.metrics_df['is_accepted']\n",
        "        )\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
        "\n",
        "        agent_accept = (agent['is_accepted'].sum() / len(agent)) * 100\n",
        "        human_accept = (human['is_accepted'].sum() / len(human)) * 100\n",
        "\n",
        "        print(f\"  Agent acceptance: {agent_accept:.1f}%\")\n",
        "        print(f\"  Human acceptance: {human_accept:.1f}%\")\n",
        "        print(f\"  Difference: {agent_accept - human_accept:.1f} pp\")\n",
        "        print(f\"  Chi-square: χ²={chi2:.2f}, p={p:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': 'acceptance_rate',\n",
        "            'Agent_Value': agent_accept,\n",
        "            'Human_Value': human_accept,\n",
        "            'Test_Statistic': chi2,\n",
        "            'p_value': p\n",
        "        })\n",
        "\n",
        "        print(\"\\n[RQ2.2] Review Dynamics:\")\n",
        "        # Review time (for closed PRs)\n",
        "        closed_metrics = self.metrics_df[self.metrics_df['is_closed'] == 1]\n",
        "        agent_closed = closed_metrics[closed_metrics['author_type'] == 'Agent']\n",
        "        human_closed = closed_metrics[closed_metrics['author_type'] == 'Human']\n",
        "\n",
        "        stat, p = mannwhitneyu(\n",
        "            agent_closed['review_time_hours'].dropna(),\n",
        "            human_closed['review_time_hours'].dropna()\n",
        "        )\n",
        "\n",
        "        print(f\"  Agent median review time: {agent_closed['review_time_hours'].median():.1f} hours\")\n",
        "        print(f\"  Human median review time: {human_closed['review_time_hours'].median():.1f} hours\")\n",
        "        print(f\"  Mann-Whitney U: U={stat:.0f}, p={p:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': 'review_time_hours',\n",
        "            'Agent_Value': agent_closed['review_time_hours'].median(),\n",
        "            'Human_Value': human_closed['review_time_hours'].median(),\n",
        "            'Test_Statistic': stat,\n",
        "            'p_value': p\n",
        "        })\n",
        "\n",
        "        print(\"\\n[RQ2.3] Review Activity:\")\n",
        "\n",
        "        # Use num_comments we already calculated (reviews + comments)\n",
        "        agent_activity = agent['num_comments'].dropna()\n",
        "        human_activity = human['num_comments'].dropna()\n",
        "\n",
        "        if len(agent_activity) > 0 and len(human_activity) > 0:\n",
        "            print(f\" Agent median review activity: {agent['num_comments'].median():.1f}\")\n",
        "            print(f\" Human median review activity: {human['num_comments'].median():.1f}\")\n",
        "            stat, p = mannwhitneyu(agent_activity, human_activity)\n",
        "            print(f\"  Mann-Whitney U: U={stat:.0f}, p={p:.4f}\")\n",
        "        else:\n",
        "            print(\"  Insufficient data in one group for review activity comparison\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': 'review_activity',\n",
        "            'Agent_Value': agent['num_comments'].median(),\n",
        "            'Human_Value': human['num_comments'].median(),\n",
        "            'Test_Statistic': stat,\n",
        "            'p_value': p\n",
        "        })\n",
        "\n",
        "        # Save results\n",
        "        rq2_df = pd.DataFrame(results)\n",
        "        rq2_df.to_csv(f'{self.output_path}rq2_results.csv', index=False)\n",
        "        print(f\"\\n✓ RQ2 results saved to {self.output_path}rq2_results.csv\")\n",
        "\n",
        "        # Visualizations\n",
        "        self._visualize_rq2()\n",
        "\n",
        "    def _visualize_rq2(self):\n",
        "        \"\"\"Generate RQ2 visualizations.\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Acceptance rates\n",
        "        accept_rates = self.metrics_df.groupby('author_type')['is_accepted'].mean() * 100\n",
        "        accept_rates.plot(kind='bar', ax=axes[0,0], color=['#ff6b6b', '#4ecdc4'])\n",
        "        axes[0,0].set_title('PR Acceptance Rate (%)')\n",
        "        axes[0,0].set_ylabel('Acceptance Rate (%)')\n",
        "        axes[0,0].set_xlabel('Author Type')\n",
        "        axes[0,0].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "        # Review time distribution (log scale for clarity)\n",
        "        closed_df = self.metrics_df[self.metrics_df['is_closed'] == 1]\n",
        "        sns.boxplot(data=closed_df, x='author_type', y='review_time_hours', ax=axes[0,1])\n",
        "        axes[0,1].set_title('Review Time Distribution (Closed PRs)')\n",
        "        axes[0,1].set_ylabel('Hours')\n",
        "        axes[0,1].set_yscale('log')\n",
        "\n",
        "        # Review activity\n",
        "        sns.boxplot(data=self.metrics_df, x='author_type', y='num_comments', ax=axes[1,0])\n",
        "        axes[1,0].set_title('Total Review Activity (Comments + Reviews)')\n",
        "        axes[1,0].set_ylabel('Count')\n",
        "        axes[1,0].set_ylim(0, 20)\n",
        "\n",
        "        # Acceptance by agent\n",
        "        agent_accept = self.metrics_df[\n",
        "            self.metrics_df['author_type'] == 'Agent'\n",
        "        ].groupby('agent')['is_accepted'].mean() * 100\n",
        "        agent_accept.plot(kind='barh', ax=axes[1,1])\n",
        "        axes[1,1].set_title('Acceptance Rate by Agent')\n",
        "        axes[1,1].set_xlabel('Acceptance Rate (%)')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.output_path}rq2_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Visualizations saved to {self.output_path}rq2_visualizations.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_rq3(self):\n",
        "        \"\"\"RQ3: Success predictors using ML.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ANALYZING RQ3: SUCCESS PREDICTORS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Prepare features for ML\n",
        "        print(\"\\nPreparing features...\")\n",
        "\n",
        "        # Select features\n",
        "        feature_cols = [\n",
        "            'loc_changed', 'files_touched', 'additions', 'deletions',\n",
        "            'complexity_score', 'change_dispersion', 'has_tests',\n",
        "            'title_length', 'body_length', 'has_description',\n",
        "            'stars', 'is_bug_fix', 'linked_issues'\n",
        "        ]\n",
        "\n",
        "        # Encode categorical features\n",
        "        ml_df = self.metrics_df.copy()\n",
        "        ml_df['language_encoded'] = ml_df['language'].astype('category').cat.codes\n",
        "        ml_df['type_encoded'] = ml_df['type'].astype('category').cat.codes\n",
        "\n",
        "        feature_cols.extend(['language_encoded', 'type_encoded'])\n",
        "\n",
        "        # Prepare X and y\n",
        "        X = ml_df[feature_cols].fillna(0)\n",
        "        y = ml_df['is_accepted']\n",
        "\n",
        "        # Remove rows with missing target\n",
        "        valid_idx = y.notna()\n",
        "        X = X[valid_idx]\n",
        "        y = y[valid_idx]\n",
        "\n",
        "        print(f\"  Features: {len(feature_cols)}\")\n",
        "        print(f\"  Samples: {len(X):,}\")\n",
        "        print(f\"  Positive class: {y.sum():,} ({y.mean()*100:.1f}%)\")\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Train Random Forest\n",
        "        print(\"\\nTraining Random Forest classifier...\")\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = rf.predict(X_test)\n",
        "        y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\n\" + \"-\" * 70)\n",
        "        print(\"MODEL PERFORMANCE\")\n",
        "        print(\"-\" * 70)\n",
        "        print(classification_report(y_test, y_pred, target_names=['Rejected', 'Accepted']))\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
        "        print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        print(\"\\n\" + \"-\" * 70)\n",
        "        print(\"FEATURE IMPORTANCE\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_cols,\n",
        "            'Importance': rf.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        print(importance_df.head(10).to_string(index=False))\n",
        "\n",
        "        # Save results\n",
        "        importance_df.to_csv(f'{self.output_path}rq3_feature_importance.csv', index=False)\n",
        "\n",
        "        # Cross-validation\n",
        "        print(\"\\nPerforming 5-fold cross-validation...\")\n",
        "        cv_scores = cross_val_score(rf, X, y, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "        print(f\"  CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "        # Save model performance\n",
        "        results = {\n",
        "            'Metric': ['Accuracy', 'ROC-AUC', 'CV ROC-AUC Mean', 'CV ROC-AUC Std'],\n",
        "            'Value': [accuracy, roc_auc, cv_scores.mean(), cv_scores.std()]\n",
        "        }\n",
        "        pd.DataFrame(results).to_csv(f'{self.output_path}rq3_model_performance.csv', index=False)\n",
        "\n",
        "        print(f\"\\n✓ RQ3 results saved\")\n",
        "\n",
        "        # Visualizations\n",
        "        self._visualize_rq3(importance_df, y_test, y_pred)\n",
        "\n",
        "    def _visualize_rq3(self, importance_df, y_test, y_pred):\n",
        "        \"\"\"Generate RQ3 visualizations.\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Feature importance (top 10)\n",
        "        top_features = importance_df.head(10)\n",
        "        axes[0,0].barh(top_features['Feature'], top_features['Importance'])\n",
        "        axes[0,0].set_xlabel('Importance')\n",
        "        axes[0,0].set_title('Top 10 Feature Importances')\n",
        "        axes[0,0].invert_yaxis()\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
        "        axes[0,1].set_xlabel('Predicted')\n",
        "        axes[0,1].set_ylabel('Actual')\n",
        "        axes[0,1].set_title('Confusion Matrix')\n",
        "        axes[0,1].set_xticklabels(['Rejected', 'Accepted'])\n",
        "        axes[0,1].set_yticklabels(['Rejected', 'Accepted'])\n",
        "\n",
        "        # Acceptance by task type\n",
        "        accept_by_task = self.metrics_df.groupby('type')['is_accepted'].mean() * 100\n",
        "        accept_by_task.sort_values(ascending=False).plot(kind='barh', ax=axes[1,0])\n",
        "        axes[1,0].set_xlabel('Acceptance Rate (%)')\n",
        "        axes[1,0].set_title('Acceptance Rate by Task Type')\n",
        "\n",
        "        # PR size vs acceptance\n",
        "        self.metrics_df['size_category'] = pd.cut(\n",
        "            self.metrics_df['loc_changed'],\n",
        "            bins=[0, 50, 200, 1000, float('inf')],\n",
        "            labels=['Small (<50)', 'Medium (50-200)', 'Large (200-1000)', 'XLarge (>1000)']\n",
        "        )\n",
        "        size_accept = self.metrics_df.groupby('size_category')['is_accepted'].mean() * 100\n",
        "        size_accept.plot(kind='bar', ax=axes[1,1])\n",
        "        axes[1,1].set_xlabel('PR Size')\n",
        "        axes[1,1].set_ylabel('Acceptance Rate (%)')\n",
        "        axes[1,1].set_title('Acceptance Rate by PR Size')\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.output_path}rq3_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Visualizations saved to {self.output_path}rq3_visualizations.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        \"\"\"Generate executive summary.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"GENERATING EXECUTIVE SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        summary = []\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"AIDEV RESEARCH ANALYSIS - EXECUTIVE SUMMARY\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        # Dataset\n",
        "        summary.append(\"DATASET SUMMARY\")\n",
        "        summary.append(\"-\" * 70)\n",
        "        summary.append(f\"Total PRs: {len(self.metrics_df):,}\")\n",
        "\n",
        "        agent = self.metrics_df[self.metrics_df['author_type'] == 'Agent']\n",
        "        human = self.metrics_df[self.metrics_df['author_type'] == 'Human']\n",
        "\n",
        "        summary.append(f\"Agent PRs: {len(agent):,} ({len(agent)/len(self.metrics_df)*100:.1f}%)\")\n",
        "        summary.append(f\"Human PRs: {len(human):,} ({len(human)/len(self.metrics_df)*100:.1f}%)\")\n",
        "        summary.append(f\"Repositories: {self.metrics_df['repo_id'].nunique():,}\")\n",
        "        summary.append(f\"Languages: {self.metrics_df['language'].nunique()}\")\n",
        "\n",
        "        # Key Findings\n",
        "        summary.append(\"\\n\\nKEY FINDINGS\")\n",
        "        summary.append(\"-\" * 70)\n",
        "\n",
        "        # RQ1\n",
        "        summary.append(\"\\n[RQ1] Structural Code Quality:\")\n",
        "        summary.append(f\"  • Agent median LOC: {agent['loc_changed'].median():.0f} vs Human: {human['loc_changed'].median():.0f}\")\n",
        "        summary.append(f\"  • Agent median files: {agent['files_touched'].median():.0f} vs Human: {human['files_touched'].median():.0f}\")\n",
        "        summary.append(f\"  • Agent test inclusion: {(agent['has_tests'].sum()/len(agent)*100):.1f}% vs Human: {(human['has_tests'].sum()/len(human)*100):.1f}%\")\n",
        "\n",
        "        # RQ2\n",
        "        summary.append(\"\\n[RQ2] Maintenance Effort:\")\n",
        "        summary.append(f\"  • Agent acceptance: {(agent['is_accepted'].sum()/len(agent)*100):.1f}% vs Human: {(human['is_accepted'].sum()/len(human)*100):.1f}%\")\n",
        "\n",
        "        closed_agent = agent[agent['is_closed'] == 1]\n",
        "        closed_human = human[human['is_closed'] == 1]\n",
        "        summary.append(f\"  • Agent review time: {closed_agent['review_time_hours'].median():.1f}h vs Human: {closed_human['review_time_hours'].median():.1f}h\")\n",
        "        summary.append(f\" • Agent review activity: {agent['num_comments'].median():.1f} vs Human: {human['num_comments'].median():.1f}\")\n",
        "\n",
        "        # RQ3\n",
        "        summary.append(\"\\n[RQ3] Success Predictors:\")\n",
        "        summary.append(\"  • See feature importance CSV for detailed rankings\")\n",
        "        summary.append(f\"  • Model accuracy: See RQ3 results\")\n",
        "\n",
        "        summary.append(\"\\n\" + \"=\" * 70)\n",
        "        summary.append(\"RECOMMENDATIONS\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"1. Developers should focus on test inclusion when using AI agents\")\n",
        "        summary.append(\"2. Review processes need adaptation for AI-generated code\")\n",
        "        summary.append(\"3. AI agents excel at documentation but need improvement in complex tasks\")\n",
        "        summary.append(\"4. Tool designers should optimize for acceptance, not just speed\")\n",
        "        summary.append(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "        summary_text = \"\\n\".join(summary)\n",
        "        print(summary_text)\n",
        "\n",
        "        with open(f'{self.output_path}executive_summary.txt', 'w') as f:\n",
        "            f.write(summary_text)\n",
        "\n",
        "        print(f\"\\n✓ Summary saved to {self.output_path}executive_summary.txt\")"
      ],
      "metadata": {
        "id": "KWouUJLifqnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyGT20ZG_cKO",
        "outputId": "47097396-8522-43fb-b747-6d6250e2a4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries imported successfully\n",
            "Timestamp: 2026-01-04 00:22:24\n",
            "\n",
            "======================================================================\n",
            "AIDEV ANALYSIS PIPELINE - START\n",
            "======================================================================\n",
            "Timestamp: 2026-01-04 00:22:24\n",
            "======================================================================\n",
            "AIDevAnalyzer Initialized\n",
            "======================================================================\n",
            "Output path: ./outputs/\n",
            "\n",
            "\n",
            "======================================================================\n",
            "STEP 1: LOADING DATASET\n",
            "======================================================================\n",
            "\n",
            "[1/12] Loading pull_request (pop)...\n",
            "[2/12] Loading repository (pop)...\n",
            "[3/12] Loading user (pop)...\n",
            "[4/12] Loading pr_commits (pop)...\n",
            "[5/12] Loading pr_commit_details (pop)...\n",
            "[6/12] Loading pr_reviews (pop)...\n",
            "[7/12] Loading pr_comments (pop)...\n",
            "[8/12] Loading pr_task_type (pop)...\n",
            "[9/12] Loading related_issue (pop)...\n",
            "[10/12] Loading issue (pop)...\n",
            "[11/12] Loading pr_timeline (pop)...\n",
            "[12/12] Loading human_pull_request (base)...\n",
            "\n",
            "✓ All tables loaded successfully!\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "DATASET OVERVIEW\n",
            "----------------------------------------------------------------------\n",
            "Agentic PRs (pop): 33,596\n",
            "Human PRs (base): 6,618\n",
            "Repositories: 2,807\n",
            "Users: 1,796\n",
            "Commits: 88,576\n",
            "Commit Details: 711,923\n",
            "Reviews: 28,875\n",
            "Comments: 39,122\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "STEP 2: APPLYING INCLUSION/EXCLUSION CRITERIA\n",
            "======================================================================\n",
            "\n",
            "Initial Agentic PRs: 33,596\n",
            "  → After star filter: 12,373 PRs\n",
            "  → After date filter: 7,341 PRs\n",
            "  → After completeness filter: 7,333 PRs\n",
            "\n",
            "Initial Human PRs: 6,618\n",
            "  → After filters: 6,249 PRs\n",
            "\n",
            "✓ Final combined dataset: 13,582 PRs\n",
            "  - Agent: 7,333 (54.0%)\n",
            "  - Human: 6,249 (46.0%)\n",
            "\n",
            "======================================================================\n",
            "STEP 3: CALCULATING METRICS\n",
            "======================================================================\n",
            "\n",
            "Merging task types...\n",
            "Processing timestamps...\n",
            "Aggregating commit details...\n",
            "Calculating RQ1 metrics (code quality)...\n",
            "  - Detecting test files...\n",
            "Calculating RQ2 metrics (maintenance effort)...\n",
            "Calculating RQ3 metrics (success predictors)...\n",
            "\n",
            "✓ Metrics calculated for 13,582 PRs\n",
            "  Total features: 44\n",
            "\n",
            "======================================================================\n",
            "ANALYZING RQ1: STRUCTURAL CODE QUALITY\n",
            "======================================================================\n",
            "\n",
            "[RQ1.1] Code Complexity Comparison:\n",
            "  loc_changed: Agent=103.0, Human=0.0, p=0.0000\n",
            "  files_touched: Agent=3.0, Human=0.0, p=0.0000\n",
            "  additions: Agent=72.0, Human=0.0, p=0.0000\n",
            "  deletions: Agent=16.0, Human=0.0, p=0.0000\n",
            "  complexity_score: Agent=15.3, Human=0.0, p=0.0000\n",
            "  change_dispersion: Agent=0.0, Human=0.0, p=0.0000\n",
            "\n",
            "[RQ1.2] Testing Practices:\n",
            "  Agent with tests: 44.2%\n",
            "  Human with tests: 0.0%\n",
            "  Chi-square: χ²=3626.55, p=0.0000\n",
            "\n",
            "[RQ1.3] Task Type Distribution:\n",
            "type         build  chore   ci  docs  feat   fix  other  perf  refactor  \\\n",
            "author_type                                                               \n",
            "Agent          1.9    4.7  1.0  12.6  31.6  32.9    0.1   1.2       8.5   \n",
            "Human          9.3   12.9  1.5   7.9  29.3  26.8    1.7   1.3       5.5   \n",
            "\n",
            "type         revert  style  test  \n",
            "author_type                       \n",
            "Agent           0.0    0.4   5.1  \n",
            "Human           0.0    0.9   2.8  \n",
            "\n",
            "✓ RQ1 results saved to ./outputs/rq1_results.csv\n",
            "✓ Visualizations saved to ./outputs/rq1_visualizations.png\n",
            "\n",
            "======================================================================\n",
            "ANALYZING RQ2: MAINTENANCE EFFORT\n",
            "======================================================================\n",
            "\n",
            "[RQ2.1] Acceptance Rates:\n",
            "  Agent acceptance: 55.8%\n",
            "  Human acceptance: 77.6%\n",
            "  Difference: -21.8 pp\n",
            "  Chi-square: χ²=713.07, p=0.0000\n",
            "\n",
            "[RQ2.2] Review Dynamics:\n",
            "  Agent median review time: 4.1 hours\n",
            "  Human median review time: 5.6 hours\n",
            "  Mann-Whitney U: U=19626873, p=0.1390\n",
            "\n",
            "[RQ2.3] Review Activity:\n",
            " Agent median review activity: 2.0\n",
            " Human median review activity: 0.0\n",
            "  Mann-Whitney U: U=41140292, p=0.0000\n",
            "\n",
            "✓ RQ2 results saved to ./outputs/rq2_results.csv\n",
            "✓ Visualizations saved to ./outputs/rq2_visualizations.png\n",
            "\n",
            "======================================================================\n",
            "ANALYZING RQ3: SUCCESS PREDICTORS\n",
            "======================================================================\n",
            "\n",
            "Preparing features...\n",
            "  Features: 15\n",
            "  Samples: 13,582\n",
            "  Positive class: 8,939 (65.8%)\n",
            "\n",
            "Training Random Forest classifier...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "MODEL PERFORMANCE\n",
            "----------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Rejected       0.57      0.57      0.57      1393\n",
            "    Accepted       0.78      0.78      0.78      2682\n",
            "\n",
            "    accuracy                           0.71      4075\n",
            "   macro avg       0.67      0.67      0.67      4075\n",
            "weighted avg       0.71      0.71      0.71      4075\n",
            "\n",
            "\n",
            "Accuracy: 0.707\n",
            "ROC-AUC: 0.745\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "FEATURE IMPORTANCE\n",
            "----------------------------------------------------------------------\n",
            "          Feature  Importance\n",
            "            stars    0.145050\n",
            "      body_length    0.133524\n",
            "     title_length    0.094463\n",
            "        additions    0.092310\n",
            " complexity_score    0.082203\n",
            " language_encoded    0.079302\n",
            "change_dispersion    0.073926\n",
            "     type_encoded    0.068308\n",
            "      loc_changed    0.066551\n",
            "        deletions    0.057704\n",
            "\n",
            "Performing 5-fold cross-validation...\n",
            "  CV ROC-AUC: 0.610 (+/- 0.234)\n",
            "\n",
            "✓ RQ3 results saved\n",
            "✓ Visualizations saved to ./outputs/rq3_visualizations.png\n",
            "\n",
            "======================================================================\n",
            "GENERATING EXECUTIVE SUMMARY\n",
            "======================================================================\n",
            "======================================================================\n",
            "AIDEV RESEARCH ANALYSIS - EXECUTIVE SUMMARY\n",
            "======================================================================\n",
            "Generated: 2026-01-04 00:23:41\n",
            "\n",
            "DATASET SUMMARY\n",
            "----------------------------------------------------------------------\n",
            "Total PRs: 13,582\n",
            "Agent PRs: 7,333 (54.0%)\n",
            "Human PRs: 6,249 (46.0%)\n",
            "Repositories: 870\n",
            "Languages: 51\n",
            "\n",
            "\n",
            "KEY FINDINGS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "[RQ1] Structural Code Quality:\n",
            "  • Agent median LOC: 103 vs Human: 0\n",
            "  • Agent median files: 3 vs Human: 0\n",
            "  • Agent test inclusion: 44.2% vs Human: 0.0%\n",
            "\n",
            "[RQ2] Maintenance Effort:\n",
            "  • Agent acceptance: 55.8% vs Human: 77.6%\n",
            "  • Agent review time: 4.1h vs Human: 5.6h\n",
            " • Agent review activity: 2.0 vs Human: 0.0\n",
            "\n",
            "[RQ3] Success Predictors:\n",
            "  • See feature importance CSV for detailed rankings\n",
            "  • Model accuracy: See RQ3 results\n",
            "\n",
            "======================================================================\n",
            "RECOMMENDATIONS\n",
            "======================================================================\n",
            "1. Developers should focus on test inclusion when using AI agents\n",
            "2. Review processes need adaptation for AI-generated code\n",
            "3. AI agents excel at documentation but need improvement in complex tasks\n",
            "4. Tool designers should optimize for acceptance, not just speed\n",
            "\n",
            "======================================================================\n",
            "\n",
            "✓ Summary saved to ./outputs/executive_summary.txt\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "All outputs saved to: ./outputs/\n",
            "\n",
            "Generated files:\n",
            "  - filtered_dataset.csv\n",
            "  - rq1_results.csv\n",
            "  - rq1_visualizations.png\n",
            "  - rq2_results.csv\n",
            "  - rq2_visualizations.png\n",
            "  - rq3_feature_importance.csv\n",
            "  - rq3_model_performance.csv\n",
            "  - rq3_visualizations.png\n",
            "  - executive_summary.txt\n"
          ]
        }
      ],
      "source": [
        "# %% Main execution pipeline\n",
        "def main():\n",
        "    \"\"\"Execute complete research pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"AIDEV ANALYSIS PIPELINE - START\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize\n",
        "        analyzer = AIDevAnalyzer(output_path='./outputs/')\n",
        "\n",
        "        # Create output directory\n",
        "        import os\n",
        "        os.makedirs(analyzer.output_path, exist_ok=True)\n",
        "\n",
        "        # Execute pipeline\n",
        "        analyzer.load_data()\n",
        "        analyzer.apply_inclusion_exclusion_criteria(min_stars=500)\n",
        "        analyzer.calculate_metrics()\n",
        "        analyzer.analyze_rq1()\n",
        "        analyzer.analyze_rq2()\n",
        "        analyzer.analyze_rq3()\n",
        "        analyzer.generate_summary_report()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\nAll outputs saved to: {analyzer.output_path}\")\n",
        "        print(\"\\nGenerated files:\")\n",
        "        print(\"  - filtered_dataset.csv\")\n",
        "        print(\"  - rq1_results.csv\")\n",
        "        print(\"  - rq1_visualizations.png\")\n",
        "        print(\"  - rq2_results.csv\")\n",
        "        print(\"  - rq2_visualizations.png\")\n",
        "        print(\"  - rq3_feature_importance.csv\")\n",
        "        print(\"  - rq3_model_performance.csv\")\n",
        "        print(\"  - rq3_visualizations.png\")\n",
        "        print(\"  - executive_summary.txt\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "# %% Run pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    exit_code = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cjre7D3FtQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
